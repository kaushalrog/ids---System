UBUNTU/LINUX STEP-BY-STEP EXECUTION GUIDE
========================================================================

PREREQUISITE STEPS (Ubuntu/Linux):
========================================================================

Step 0.1: Verify Python is installed
---
Run in Terminal:
    python3 --version

Expected output: Python 3.x.x (should be 3.7+)

If not installed:
    sudo apt update
    sudo apt install python3 python3-pip -y


Step 0.2: Install required packages
---
Run in Terminal:
    pip3 install --upgrade pandas numpy scikit-learn scipy matplotlib seaborn

Or with sudo if needed:
    sudo pip3 install --upgrade pandas numpy scikit-learn scipy matplotlib seaborn

Verify installation:
    python3 -c "import pandas, numpy, sklearn, scipy, matplotlib, seaborn; print('All packages OK')"


Step 0.3: Navigate to project directory
---
Run in Terminal:
    cd ~/Downloads/ids-system
    # or wherever your project is located
    pwd  # Verify you're in correct directory

If project is in different location:
    cd /path/to/your/ids-system
    ls -la  # List files to verify


Step 0.4: Verify drift_log.csv exists
---
Run in Terminal:
    test -f drift_log.csv && echo "File exists" || echo "File not found"

Should return: File exists

If not found, generate it first:
    python3 app.py &
    python3 generate_baseline.py
    python3 online_monitor.py


========================================================================
MAIN EXECUTION STEPS (Ubuntu/Linux):
========================================================================

STEP 1: Run Improved Results Analyzer
---
Command:
    python3 improved_results_analyzer.py

What happens:
  ✓ Analyzes your drift_log.csv
  ✓ Calculates 10+ performance metrics
  ✓ Generates visualizations
  ✓ Creates CSV reports

Expected output:
  ==================
  IMPROVED IDS ACCURACY ANALYSIS - COMPREHENSIVE REPORT
  ==================
  [1] CORE PERFORMANCE METRICS
  [accuracy, precision, recall, F1 scores...]
  [2] CONFIDENCE INTERVALS...
  [3] CONFUSION MATRIX...
  [etc...]

Files generated:
  → improved_metrics_summary.csv
  → improved_phase_analysis.csv
  → improved_results_detailed.csv
  → improved_roc_curve.png
  → improved_precision_recall.png
  → improved_confusion_matrix.png
  → improved_drift_distribution.png
  → improved_metrics_comparison.png

Time: ~10-15 seconds


STEP 2: Run Advanced Threshold Optimizer
---
Command:
    python3 advanced_threshold_optimizer.py

What happens:
  ✓ Tests 5 different threshold optimization strategies
  ✓ Compares F1, Youden, Balanced Accuracy, Cost-Sensitive, ROC
  ✓ Recommends best ensemble threshold

Expected output:
  ============================================
  ADVANCED THRESHOLD OPTIMIZATION
  ============================================
  [Strategy 1] F1-Score Optimization
    Optimal Threshold: 0.6234
    F1-Score: 0.8945
    Metrics: {...}
  
  [Strategy 2] Youden Index Optimization
    ...
  
  [Strategy 3] Balanced Accuracy Optimization
    ...
  
  [Strategy 4] Cost-Sensitive Learning
    ...
  
  [Strategy 5] ROC Optimal Point
    ...

Files generated:
  → optimized_thresholds.json
  → threshold_optimization_curves.png

Time: ~20-30 seconds

IMPORTANT: Note the ensemble_recommendation value!


STEP 3: Run Comprehensive Accuracy Report
---
Command:
    python3 comprehensive_accuracy_report.py

What happens:
  ✓ Statistical analysis (t-tests, KS tests)
  ✓ Attack pattern detection
  ✓ Detection efficiency analysis
  ✓ Model robustness assessment

Expected output:
  ==========================================
  COMPREHENSIVE IDS ACCURACY REPORT
  ==========================================
  [1] STATISTICAL ANOMALY ANALYSIS
    Normal_Mean........... 1.2345
    Anomaly_Mean......... 3.4567
    Cohen_D............. 0.9876
    [etc...]
  
  [2] ATTACK PATTERN DETECTION
    High_Drift_Samples... 45
    Alert_Bursts......... 12
    [etc...]
  
  [3] DETECTION EFFICIENCY METRICS
    Alert_Purity........ 0.92
    Attack_Coverage.... 0.88
    [etc...]
  
  [4] MODEL ROBUSTNESS...
    Consistency_Score... 0.85
    [etc...]

Files generated:
  → comprehensive_accuracy_report.json
  → comprehensive_accuracy_analysis.png

Time: ~15-20 seconds


========================================================================
OPTION A: RUN EACH STEP INDIVIDUALLY
========================================================================

# Terminal commands - run each one separately and wait for completion

python3 improved_results_analyzer.py
# Wait for completion...

python3 advanced_threshold_optimizer.py
# Wait for completion...

python3 comprehensive_accuracy_report.py
# Wait for completion...


========================================================================
OPTION B: RUN ALL 3 SEQUENTIALLY (Best)
========================================================================

Run this single command - all 3 will run one after another:

    python3 improved_results_analyzer.py && python3 advanced_threshold_optimizer.py && python3 comprehensive_accuracy_report.py

Advantages:
  ✓ Runs all 3 automatically
  ✓ Stops if any script fails
  ✓ Total time: ~45-65 seconds
  ✓ No manual waiting needed

Alternative syntax (all will run even if one fails):
    python3 improved_results_analyzer.py; python3 advanced_threshold_optimizer.py; python3 comprehensive_accuracy_report.py


========================================================================
OPTION C: RUN IN BACKGROUND (Advanced)
========================================================================

Run all 3 in background and let them complete:

    nohup python3 improved_results_analyzer.py > step1.log 2>&1 &
    nohup python3 advanced_threshold_optimizer.py > step2.log 2>&1 &
    nohup python3 comprehensive_accuracy_report.py > step3.log 2>&1 &

Check progress:
    tail -f step1.log
    tail -f step2.log
    tail -f step3.log

Check if finished:
    jobs
    ps aux | grep python3


========================================================================
VERIFY ALL FILES WERE CREATED
========================================================================

After all steps complete, run:

    ls -lh improved_*.* optimized_*.* comprehensive_*.*

Or count them:
    ls improved_*.* optimized_*.* comprehensive_*.* | wc -l

Should show 12 files total


========================================================================
VIEW RESULTS
========================================================================

View CSV files (metrics):
    cat improved_metrics_summary.csv
    # or
    cat improved_phase_analysis.csv

View JSON files (thresholds):
    cat optimized_thresholds.json
    # or
    cat comprehensive_accuracy_report.json

Pretty-print JSON:
    python3 -m json.tool optimized_thresholds.json
    python3 -m json.tool comprehensive_accuracy_report.json

View PNG files (open in image viewer):
    # GUI method (if you have a desktop environment)
    eog improved_roc_curve.png &
    eog improved_confusion_matrix.png &
    
    # Or use your default image viewer
    xdg-open improved_roc_curve.png

Or copy to view on Windows/Mac:
    scp user@ubuntu-ip:~/Downloads/ids-system/*.png /local/path/


========================================================================
INTERPRETING RESULTS - KEY METRICS
========================================================================

FROM STEP 1 (Improved Results Analyzer):
---
Look for these values in console output:

Accuracy:       Should be > 0.85 (85%)
                ✓ Good if > 0.90
                ✗ Bad if < 0.80

Precision:      Should be > 0.80
                ✓ What % of your alerts are real attacks
                ✗ High FP rate if too low

Recall:         Should be > 0.85
                ✓ What % of attacks you caught
                ✗ Missing attacks if too low

F1-Score:       Should be > 0.85
                ✓ Balance between precision and recall
                
Confidence Intervals (95% CI):
                ✓ Narrow = more reliable
                ✗ Wide = results uncertain


FROM STEP 2 (Advanced Threshold Optimizer):
---
Look for in console output:

F1-Score Optimization: Best balanced threshold
Youden Index:         Best TPR - FPR balance
Balanced Accuracy:    Best for imbalanced data
Cost-Sensitive:       Best if FN more costly
ROC Optimal:          Best overall separation

ensemble_recommendation: MOST IMPORTANT!
                ✓ Use this value as your new threshold
                ✓ Example: 0.65 → set WARNING_THRESHOLD = 0.65


FROM STEP 3 (Comprehensive Accuracy Report):
---
Look for in console output:

Cohen's D:              > 0.8 = strong separation
                        ✓ Values > 1.0 = excellent

Consistency_Score:     > 0.8 = robust model
                        ✓ Model performs uniformly

Time_To_Detection:     Fewer samples = faster detection
                        ✓ < 10 samples = excellent

Alert_Purity:          > 0.90 = good (few false alarms)
                        ✗ < 0.70 = too many false alerts

Attack_Coverage:       > 0.90 = catching most attacks
                        ✗ < 0.70 = missing too many


========================================================================
COMMON ERRORS & FIXES (Ubuntu/Linux)
========================================================================

ERROR: "python3: command not found"
---
Fix:
    sudo apt update
    sudo apt install python3 python3-pip -y
    python3 --version


ERROR: "ModuleNotFoundError: No module named 'pandas'"
---
Fix:
    pip3 install --upgrade pandas numpy scikit-learn scipy matplotlib seaborn


ERROR: "FileNotFoundError: [Errno 2] No such file or directory: 'drift_log.csv'"
---
Fix:
    1. Check if file exists:
        ls -la drift_log.csv
    2. If not, generate it first:
        python3 app.py &
        python3 generate_baseline.py
        python3 online_monitor.py


ERROR: "Permission denied" when running script
---
Fix:
    chmod +x improved_results_analyzer.py
    python3 improved_results_analyzer.py


ERROR: "Matplotlib can't display plots (running headless)"
---
This is OK! The scripts save PNG files instead.
Check for PNG files:
    ls -lh *.png


ERROR: "ImportError: cannot import name '_strptime'"
---
Fix:
    python3 -m pip install --force-reinstall python-dateutil


========================================================================
ENVIRONMENT SETUP (Optional but Recommended)
========================================================================

Create a Python virtual environment (best practice):

1. Create venv:
    python3 -m venv ids_env

2. Activate venv:
    source ids_env/bin/activate

3. Install packages in venv:
    pip install --upgrade pandas numpy scikit-learn scipy matplotlib seaborn

4. Run scripts (venv already active):
    python3 improved_results_analyzer.py
    python3 advanced_threshold_optimizer.py
    python3 comprehensive_accuracy_report.py

5. Deactivate when done:
    deactivate


========================================================================
COMPLETE UBUNTU WORKFLOW (Copy & Paste)
========================================================================

# Step 1: Prerequisites
python3 --version
sudo apt update && sudo apt install python3 python3-pip -y
pip3 install --upgrade pandas numpy scikit-learn scipy matplotlib seaborn

# Step 2: Navigate to project
cd ~/Downloads/ids-system
ls -la drift_log.csv

# Step 3: Run all analyses
python3 improved_results_analyzer.py && python3 advanced_threshold_optimizer.py && python3 comprehensive_accuracy_report.py

# Step 4: Verify results
ls -lh improved_*.* optimized_*.* comprehensive_*.*

# Step 5: View results
cat improved_metrics_summary.csv
python3 -m json.tool optimized_thresholds.json
python3 -m json.tool comprehensive_accuracy_report.json


========================================================================
TIMING EXPECTATIONS (Ubuntu/Linux)
========================================================================

Step 1 (Improved Results Analyzer):    10-15 seconds
Step 2 (Advanced Threshold Optimizer): 20-30 seconds
Step 3 (Comprehensive Report):         15-20 seconds
                                       ──────────────
TOTAL (all 3):                         45-65 seconds


========================================================================
OUTPUT FILES SUMMARY
========================================================================

After all 3 steps, you'll have:

From Step 1 (8 files):
  improved_metrics_summary.csv              (key metrics in table)
  improved_phase_analysis.csv               (phase-by-phase breakdown)
  improved_results_detailed.csv             (detailed predictions)
  improved_roc_curve.png                    (ROC visualization)
  improved_precision_recall.png             (PR curve)
  improved_confusion_matrix.png             (confusion matrix heatmap)
  improved_drift_distribution.png           (drift score distribution)
  improved_metrics_comparison.png           (metrics bar chart)

From Step 2 (2 files):
  optimized_thresholds.json                 (5 threshold strategies)
  threshold_optimization_curves.png         (threshold curves)

From Step 3 (2 files):
  comprehensive_accuracy_report.json        (statistical analysis)
  comprehensive_accuracy_analysis.png       (8-panel visualization)

TOTAL: 12 output files


========================================================================
NEXT STEPS AFTER ANALYSIS
========================================================================

1. Review metrics:
    cat improved_metrics_summary.csv | head -20

2. Check optimal threshold:
    python3 -m json.tool optimized_thresholds.json | grep -A2 "ensemble_recommendation"

3. Update your IDS with new threshold:
    Edit online_monitor.py and update warning_threshold and alert_threshold

4. Re-run with new thresholds:
    python3 app.py &
    python3 online_monitor.py
    
5. Generate new drift_log.csv and re-analyze:
    python3 improved_results_analyzer.py
    # Compare metrics to baseline


========================================================================
HELPFUL UBUNTU COMMANDS
========================================================================

# Check current directory
pwd

# List all Python files
ls -la *.py

# List all output files
ls -lh improved_*.* optimized_*.* comprehensive_*.*

# Check file sizes
du -sh *

# View file with line numbers
cat -n improved_metrics_summary.csv

# Search in files
grep "Accuracy" improved_metrics_summary.csv

# Compare two files
diff optimized_thresholds.json optimized_thresholds_old.json

# Create backup
cp drift_log.csv drift_log.csv.backup

# Monitor processes
watch -n 1 'ps aux | grep python3'

# Kill a running script
pkill -f improved_results_analyzer.py

# Check available disk space
df -h

# Archive results
tar -czf ids_results_$(date +%Y%m%d).tar.gz *.csv *.json *.png


========================================================================

Ready? Start with step 0.1 to verify Python, then follow the workflow!
