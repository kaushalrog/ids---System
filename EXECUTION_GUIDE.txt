STEP-BY-STEP GUIDE TO RUN IMPROVED IDS ACCURACY ANALYSIS
========================================================================

PREREQUISITE STEPS:
========================================================================

Step 0.1: Verify Python is installed
---
Run in PowerShell:
    python --version

Expected output: Python 3.x.x (should be 3.7+)


Step 0.2: Verify required packages are installed
---
Run in PowerShell:
    python -m pip list

Should see: pandas, numpy, scikit-learn, scipy, matplotlib, seaborn

If missing, install:
    python -m pip install pandas numpy scikit-learn scipy matplotlib seaborn --upgrade


Step 0.3: Navigate to project directory
---
Run in PowerShell:
    cd c:\Users\kaush\Downloads\ids-system
    pwd  # Verify you're in correct directory


Step 0.4: Verify drift_log.csv exists (required input file)
---
Run in PowerShell:
    Test-Path drift_log.csv

Should return: True

If False, you need to:
  1. Run your app.py to collect telemetry
  2. Run generate_baseline.py to create baseline
  3. Run online_monitor.py to generate drift_log.csv


========================================================================
MAIN EXECUTION STEPS:
========================================================================

STEP 1: Run Improved Results Analyzer
---
Purpose: Comprehensive metrics with confidence intervals and phase analysis
Time: ~10-15 seconds

Run this command:
    python improved_results_analyzer.py

What it does:
  ✓ Calculates 10+ performance metrics (Accuracy, Precision, Recall, F1, etc.)
  ✓ Generates 95% confidence intervals for reliability
  ✓ Analyzes performance by time phases
  ✓ Finds optimal decision threshold
  ✓ Creates 5 visualizations

Expected console output:
  - CORE PERFORMANCE METRICS table
  - CONFIDENCE INTERVALS (95%)
  - CONFUSION MATRIX
  - TEMPORAL PHASE ANALYSIS
  - OPTIMAL THRESHOLD ANALYSIS
  - CLASS DISTRIBUTION

Files generated:
  → improved_metrics_summary.csv (summary table)
  → improved_phase_analysis.csv (phase-by-phase metrics)
  → improved_results_detailed.csv (detailed results)
  → improved_roc_curve.png (visualization)
  → improved_precision_recall.png (visualization)
  → improved_confusion_matrix.png (visualization)
  → improved_drift_distribution.png (visualization)
  → improved_metrics_comparison.png (visualization)


STEP 2: Run Advanced Threshold Optimizer
---
Purpose: Find optimal alert thresholds using 5 different strategies
Time: ~20-30 seconds

Run this command:
    python advanced_threshold_optimizer.py

What it does:
  ✓ Compares F1-Score optimization
  ✓ Finds Youden Index optimal point
  ✓ Balances Accuracy optimization
  ✓ Applies Cost-Sensitive Learning
  ✓ Finds ROC optimal point
  ✓ Recommends ensemble threshold

Expected console output:
  - [Strategy 1] F1-Score Optimization
  - [Strategy 2] Youden Index Optimization
  - [Strategy 3] Balanced Accuracy Optimization
  - [Strategy 4] Cost-Sensitive Learning
  - [Strategy 5] ROC Optimal Point

Files generated:
  → optimized_thresholds.json (all 5 strategies + ensemble)
  → threshold_optimization_curves.png (metric curves across thresholds)


STEP 3: Run Comprehensive Accuracy Report
---
Purpose: Statistical analysis with attack pattern detection
Time: ~15-20 seconds

Run this command:
    python comprehensive_accuracy_report.py

What it does:
  ✓ Performs statistical tests (t-test, KS test)
  ✓ Detects attack patterns and bursts
  ✓ Calculates detection efficiency metrics
  ✓ Assesses model robustness across time
  ✓ Computes Cohen's D effect size
  ✓ Creates 8-panel comprehensive visualization

Expected console output:
  - STATISTICAL ANOMALY ANALYSIS
  - ATTACK PATTERN DETECTION
  - DETECTION EFFICIENCY METRICS
  - MODEL ROBUSTNESS ACROSS DATA SEGMENTS
  - ACCURACY SUMMARY
  - Data statistics

Files generated:
  → comprehensive_accuracy_report.json (full report in JSON)
  → comprehensive_accuracy_analysis.png (8-panel visualization)


========================================================================
QUICK EXECUTION COMMAND (Run all 3 at once):
========================================================================

Run this command in PowerShell:
    python improved_results_analyzer.py; python advanced_threshold_optimizer.py; python comprehensive_accuracy_report.py

Or run individually and monitor each output carefully.


========================================================================
INTERPRETING RESULTS:
========================================================================

FROM STEP 1 (Improved Results Analyzer):
---
Key metrics to check:
  • Accuracy: Should be > 0.85 (85%)
  • Precision: % of your alerts that are real attacks (> 0.80 is good)
  • Recall: % of actual attacks you detected (> 0.90 is excellent)
  • Balanced_Accuracy: Better than raw accuracy for imbalanced data
  • Confidence Intervals: Narrow CI = more reliable results

Phases: 
  • Check if accuracy drops in any phase = potential model drift
  • Consistent performance across phases = robust model

Confusion Matrix:
  • TP (top-left): True Positives ✓
  • FP (top-right): False Alarms ✗
  • FN (bottom-left): Missed Attacks ✗
  • TN (bottom-right): Correct Normal Detection ✓


FROM STEP 2 (Advanced Threshold Optimizer):
---
Key values to check:
  • f1_optimized: Best balance between precision and recall
  • ensemble_recommendation: Average of best strategies
  • Compare which strategy gives highest Recall (catch more attacks)

Use ensemble_recommendation as your new warning threshold:
  Example: If ensemble_recommendation = 0.65, set WARNING_THRESHOLD = 0.65


FROM STEP 3 (Comprehensive Accuracy Report):
---
Key metrics to check:
  • Cohen's D: > 0.8 = strong separation between normal/attack
  • Statistical Significance: p-value < 0.05 = significant difference
  • Model Consistency Score: > 0.8 = robust across time
  • Time_To_Detection_Samples: How many samples before attack detected
  • Alert_Purity: % of alerts that are true attacks


========================================================================
STEP-BY-STEP EXAMPLES:
========================================================================

EXAMPLE EXECUTION:
---

1. Open PowerShell
2. Type: cd c:\Users\kaush\Downloads\ids-system
3. Type: python improved_results_analyzer.py
   (Wait for completion, check console output)
4. Type: python advanced_threshold_optimizer.py
   (Wait for completion, check console output)
5. Type: python comprehensive_accuracy_report.py
   (Wait for completion, check console output)
6. Type: ls improved_*.png
   (Verify all PNG files were created)
7. Type: ls optimized_thresholds.json
   (Verify threshold file exists)


FILE ORGANIZATION AFTER RUNNING:
---

Your directory will have:

ORIGINAL FILES (unchanged):
  ✓ app.py
  ✓ drift_detector.py
  ✓ online_monitor.py
  ✓ drift_log.csv (from your IDS runs)

NEW ANALYSIS FILES:
  → improved_results_analyzer.py (the script)
  → advanced_threshold_optimizer.py (the script)
  → comprehensive_accuracy_report.py (the script)

OUTPUT FILES FROM STEP 1:
  → improved_metrics_summary.csv
  → improved_phase_analysis.csv
  → improved_results_detailed.csv
  → improved_roc_curve.png
  → improved_precision_recall.png
  → improved_confusion_matrix.png
  → improved_drift_distribution.png
  → improved_metrics_comparison.png

OUTPUT FILES FROM STEP 2:
  → optimized_thresholds.json
  → threshold_optimization_curves.png

OUTPUT FILES FROM STEP 3:
  → comprehensive_accuracy_report.json
  → comprehensive_accuracy_analysis.png


========================================================================
TROUBLESHOOTING:
========================================================================

ERROR: "FileNotFoundError: drift_log.csv not found"
---
Solution: You need to generate drift_log.csv first
  1. Run: python app.py &
  2. Run: python generate_baseline.py
  3. Run: python online_monitor.py
  4. Then run the analysis scripts


ERROR: "ModuleNotFoundError: No module named 'sklearn'"
---
Solution: Install scikit-learn
  python -m pip install scikit-learn --upgrade


ERROR: "ModuleNotFoundError: No module named 'seaborn'"
---
Solution: Install seaborn
  python -m pip install seaborn --upgrade


ERROR: Script runs but produces empty CSV or JSON
---
Solution: drift_log.csv exists but might be empty
  1. Check file size: Get-Item drift_log.csv | Select-Object Length
  2. If size < 100 bytes, regenerate with online_monitor.py


ERROR: PNG files not created
---
Solution: Check matplotlib backend
  Run: python -c "import matplotlib; print(matplotlib.get_backend())"
  If not 'Agg', add this to top of each script after imports:
    import matplotlib
    matplotlib.use('Agg')


========================================================================
NEXT STEPS:
========================================================================

After running all 3 scripts:

1. REVIEW METRICS:
   - Open improved_metrics_summary.csv in Excel/Sheets
   - Check if metrics meet your requirements

2. CHECK VISUALIZATIONS:
   - Open all PNG files to visualize performance
   - Look for any anomalies or concerning patterns

3. APPLY OPTIMIZED THRESHOLDS:
   - Open optimized_thresholds.json
   - Use the "ensemble_recommendation" value
   - Update your online_monitor.py with new thresholds

4. MAKE ADJUSTMENTS:
   - If Recall < 0.90, use more aggressive threshold
   - If Precision < 0.80, use more conservative threshold
   - Test with different cost ratios in step 2

5. VALIDATE:
   - Run your IDS again with new thresholds
   - Re-run all 3 analysis scripts
   - Compare metrics to baseline


========================================================================

Questions? Check the docstrings in each script:
  python -c "import improved_results_analyzer; help(improved_results_analyzer.ImprovedIdsAnalyzer)"
