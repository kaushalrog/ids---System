================================================================================
    IDS SYSTEM IMPLEMENTATION ON RASPBERRY PI 5 (8GB) - DEBIAN
    Complete Step-by-Step Guide from Scratch
================================================================================

DATE: January 30, 2026
TARGET: Raspberry Pi 5 (8GB RAM, Debian 1.9GB version)
GOAL: Deploy production-ready IDS system on edge device
ESTIMATED TIME: 1.5-2 hours total

================================================================================
PART 1: PRE-DEPLOYMENT PLANNING
================================================================================

1.1 HARDWARE SPECIFICATIONS
---------------------------

Raspberry Pi 5 Specifications:
  ‚Ä¢ CPU: Broadcom BCM2712 (2.4 GHz, 4-core ARM Cortex-A76)
  ‚Ä¢ RAM: 8 GB LPDDR5X (sufficient for IDS)
  ‚Ä¢ Storage: Recommend 64GB+ microSD card (or USB SSD)
  ‚Ä¢ Network: Gigabit Ethernet (recommended over WiFi)
  ‚Ä¢ Power: 5V 5A USB-C (recommended)

Capacity Assessment:
  ‚úì RAM 8GB: SUFFICIENT
    ‚îú‚îÄ OS needs: 500-800 MB
    ‚îú‚îÄ Python + deps: 400-600 MB
    ‚îú‚îÄ Analyzer runtime: 200-400 MB
    ‚îú‚îÄ Available buffer: ~6 GB (GOOD)
  
  ‚úì Storage (64GB microSD):
    ‚îú‚îÄ OS + system: 5 GB
    ‚îú‚îÄ Project code: 50 MB
    ‚îú‚îÄ SRBH dataset: 200 MB
    ‚îú‚îÄ Results/logs: 500 MB - 2 GB
    ‚îú‚îÄ Available: ~56 GB (GOOD for months of data)

  ‚ö† Processing Power:
    ‚îú‚îÄ Pi 5 can process: ~500-1000 records/sec
    ‚îú‚îÄ For 24,990 records: ~30-50 seconds
    ‚îú‚îÄ Real-time monitoring: YES, feasible
    ‚îú‚îÄ Stream processing: YES, 10+ requests/sec possible

1.2 CONNECTIVITY OPTIONS
------------------------

Option A: Wired (RECOMMENDED)
  ‚úì Gigabit Ethernet direct to switch
  ‚úì Lower latency
  ‚úì More stable for 24/7 monitoring
  ‚úì Better for real-time IDS

Option B: WiFi (Not Recommended)
  ‚úó Slower than Ethernet
  ‚úó More interference
  ‚úó Battery drain if on mobile power
  ~ Use only if wired unavailable

1.3 STORAGE SETUP
-----------------

Setup Option A: MicroSD Card (EASIEST)
  Recommendation: Kingston Canvas Go Plus 64GB U3 A2 V30
  ‚Ä¢ Read: 170 MB/s
  ‚Ä¢ Write: 90 MB/s
  ‚Ä¢ Sufficient for edge deployment
  ‚Ä¢ Cost: ~$15-20

Setup Option B: USB SSD (BEST PERFORMANCE)
  Recommendation: Samsung T5 500GB
  ‚Ä¢ Read: 540 MB/s
  ‚Ä¢ Write: 500 MB/s
  ‚Ä¢ Better for data logging
  ‚Ä¢ Cost: ~$50-60
  ‚Ä¢ Setup: Boot from USB SSD (Pi 5 supports this)

Setup Option C: NVMe HAT (OPTIMAL)
  Recommendation: Argon Neo5 HAT
  ‚Ä¢ NVMe support (2230/2242/2260/2280)
  ‚Ä¢ Best performance
  ‚Ä¢ Passive cooling
  ‚Ä¢ Cost: ~$40-80 + NVMe

For this project: Option A (microSD) is sufficient

================================================================================
PART 2: INITIAL RASPBERRY PI 5 SETUP
================================================================================

2.1 INSTALL RASPBERRY PI OS (DEBIAN)
-----------------------------------

Step 1: Download Raspberry Pi Imager
  ‚Ä¢ Go to: https://www.raspberrypi.com/software/
  ‚Ä¢ Download Raspberry Pi Imager (for your OS: Windows/Mac/Linux)

Step 2: Insert microSD Card
  ‚Ä¢ Insert microSD card into reader
  ‚Ä¢ Connect to your computer

Step 3: Flash Image
  ‚Ä¢ Open Raspberry Pi Imager
  ‚Ä¢ Click "Choose Device" ‚Üí Select "Raspberry Pi 5"
  ‚Ä¢ Click "Choose OS" ‚Üí Select "Raspberry Pi OS (64-bit)"
  ‚Ä¢ Click "Choose Storage" ‚Üí Select your microSD card
  ‚Ä¢ Click "Advanced Options":
    ‚îú‚îÄ Set hostname: "ids-pi" (or preferred name)
    ‚îú‚îÄ Enable SSH: YES (checkmark)
    ‚îú‚îÄ Username: "pi"
    ‚îú‚îÄ Password: (set secure password)
    ‚îú‚îÄ Configure WiFi: (if needed)
    ‚îú‚îÄ Set locale: (your timezone)
  ‚Ä¢ Click "Write"
  ‚Ä¢ Wait ~5-10 minutes for completion

Step 4: Eject and Boot
  ‚Ä¢ Eject microSD card safely
  ‚Ä¢ Insert into Raspberry Pi 5
  ‚Ä¢ Connect Ethernet cable (wired preferred)
  ‚Ä¢ Connect USB-C power adapter
  ‚Ä¢ Wait 1-2 minutes for first boot

2.2 INITIAL SSH CONNECTION
--------------------------

From your main computer (Windows/Mac/Linux):

Windows (PowerShell):
  $ ssh pi@ids-pi.local
  (or use IP address: $ ssh pi@192.168.x.x)

Mac/Linux (Terminal):
  $ ssh pi@ids-pi.local

When prompted:
  ‚Ä¢ Username: pi
  ‚Ä¢ Password: (enter the password you set)

After connection, you should see:
  pi@ids-pi:~ $

2.3 SYSTEM UPDATE
-----------------

Update package lists:
  $ sudo apt update

Upgrade packages:
  $ sudo apt upgrade -y

Install essential tools:
  $ sudo apt install -y curl wget git build-essential

Check Python version:
  $ python3 --version
  (Should show Python 3.11+ - Debian 1.9GB includes this)

This takes ~3-5 minutes depending on update size.

================================================================================
PART 3: PYTHON ENVIRONMENT SETUP
================================================================================

3.1 INSTALL PYTHON & PIP
------------------------

Check if pip is installed:
  $ pip3 --version

If not installed:
  $ sudo apt install -y python3-pip

Upgrade pip:
  $ pip3 install --upgrade pip

Verify:
  $ pip3 --version
  (Should show pip 23.x or newer)

3.2 CREATE VIRTUAL ENVIRONMENT
------------------------------

Why Virtual Environment?
  ‚úì Isolates project dependencies
  ‚úì Prevents conflicts with system packages
  ‚úì Easier to manage versions
  ‚úì Required for production deployment

Create project directory:
  $ mkdir -p ~/ids-system
  $ cd ~/ids-system

Create virtual environment:
  $ python3 -m venv ids_env

Activate virtual environment:
  $ source ids_env/bin/activate

You should see prompt change to:
  (ids_env) pi@ids-pi:~/ids-system $

3.3 INSTALL DEPENDENCIES
------------------------

With virtual environment ACTIVATED, install required packages:

  $ pip install pandas numpy scikit-learn scipy matplotlib seaborn flask

Installation details:
  ‚Ä¢ pandas: Data manipulation (5 MB)
  ‚Ä¢ numpy: Numerical computing (15 MB)
  ‚Ä¢ scikit-learn: Machine learning (25 MB)
  ‚Ä¢ scipy: Scientific computing (30 MB)
  ‚Ä¢ matplotlib: Plotting (20 MB)
  ‚Ä¢ seaborn: Statistical plots (5 MB)
  ‚Ä¢ flask: Web framework (2 MB)
  
Total size: ~100 MB
Installation time: 5-15 minutes (depending on Pi's compilation speed)

Verify installation:
  $ python3 -c "import pandas, numpy, sklearn, scipy, matplotlib; print('All packages OK')"

Expected output:
  All packages OK

3.4 OPTIMIZE FOR RASPBERRY PI
----------------------------

Option A: Use Pre-compiled Wheels (RECOMMENDED)
  Most packages already have ARM64 wheels, pip handles automatically.

Option B: Install Optional Optimizations
  $ pip install numexpr bottleneck

  These speed up pandas operations by 2-3x on Pi.

Option C: Enable NumPy BLAS Optimization
  $ sudo apt install -y libatlas-base-dev
  $ pip install --force-reinstall numpy scipy

  This makes calculations faster using optimized linear algebra.

After optimization, verify:
  $ python3 -c "import numpy; numpy.show_config()"

================================================================================
PART 4: PROJECT FILES DEPLOYMENT
================================================================================

4.1 TRANSFER PROJECT FILES
--------------------------

Option A: Using SSH (RECOMMENDED)
  From your main computer, in PowerShell/Terminal:
  
  Windows:
    $ scp -r C:\Users\kaush\Downloads\ids-system\* pi@ids-pi.local:~/ids-system/

  Mac/Linux:
    $ scp -r ~/Downloads/ids-system/* pi@ids-pi.local:~/ids-system/

  This copies all project files to Pi's ~/ids-system directory

Option B: Using Git
  On Pi, clone your repository:
    $ cd ~/ids-system
    $ git clone <your-repo-url> .
  
  (Requires code to be in GitHub repository)

Option C: Using USB Drive
  1. Copy files to USB drive on main computer
  2. Insert USB into Raspberry Pi
  3. Mount and copy:
     $ sudo mount /dev/sda1 /mnt/usb
     $ cp -r /mnt/usb/ids-system/* ~/ids-system/
     $ sudo umount /mnt/usb

4.2 VERIFY FILES ON PI
----------------------

Connect to Pi via SSH and verify:
  $ cd ~/ids-system
  $ ls -la

You should see:
  ‚îú‚îÄ app.py
  ‚îú‚îÄ drift_detector.py
  ‚îú‚îÄ generate_baseline.py
  ‚îú‚îÄ online_monitor.py
  ‚îú‚îÄ improved_results_analyzer.py
  ‚îú‚îÄ advanced_threshold_optimizer.py
  ‚îú‚îÄ comprehensive_accuracy_report.py
  ‚îú‚îÄ srbh_training/
  ‚îÇ  ‚îú‚îÄ SRBH-20.csv
  ‚îÇ  ‚îú‚îÄ SRBH_2020.csv
  ‚îÇ  ‚îî‚îÄ srbh_processed.csv
  ‚îú‚îÄ fullproject.txt
  ‚îî‚îÄ *.md and *.txt files

Check disk usage:
  $ du -sh ~/ids-system
  (Should show ~200-300 MB)

4.3 CREATE STARTUP SCRIPT
------------------------

Create a script to activate venv and run analysis:

  $ nano ~/ids-system/run_ids.sh

Paste the following:
  #!/bin/bash
  
  # IDS System Startup Script for Raspberry Pi 5
  # Activates virtual environment and runs analysis
  
  cd ~/ids-system
  source ids_env/bin/activate
  
  # Run baseline generation
  echo "Generating baseline from SRBH data..."
  python3 generate_baseline.py
  
  # Run improved analyzer
  echo "Running improved results analyzer..."
  python3 improved_results_analyzer.py
  
  # Run threshold optimizer
  echo "Running threshold optimizer..."
  python3 advanced_threshold_optimizer.py
  
  # Run comprehensive report
  echo "Running comprehensive accuracy report..."
  python3 comprehensive_accuracy_report.py
  
  echo "Analysis complete! Check results/ folder"
  
  deactivate

Save and exit: Ctrl+X, then Y, then Enter

Make executable:
  $ chmod +x ~/ids-system/run_ids.sh

================================================================================
PART 5: LOAD SRBH DATASET ON PI
================================================================================

5.1 PREPARE SRBH DATA
--------------------

The SRBH dataset files are ~200 MB total.

On Pi, verify dataset is present:
  $ cd ~/ids-system
  $ ls -lh srbh_training/
  
Expected files:
  ‚Ä¢ SRBH-20.csv (~50 MB)
  ‚Ä¢ SRBH_2020.csv (~150 MB)

5.2 OPTIMIZE DATASET FOR PI
---------------------------

Create a data preprocessing script:

  $ nano ~/ids-system/preprocess_for_pi.py

Paste the following:
  #!/usr/bin/env python3
  """
  Preprocess SRBH dataset for Raspberry Pi
  Reduces memory footprint by using data types efficiently
  """
  
  import pandas as pd
  import numpy as np
  import os
  
  print("Loading SRBH datasets...")
  
  # Load with optimized data types
  dtypes = {
      'source_ip': 'category',
      'endpoint': 'category',
      'method': 'category',
      'status_code': 'int16',
      'response_time': 'float32',
      'bytes_sent': 'int32',
      'label': 'int8'
  }
  
  df1 = pd.read_csv('srbh_training/SRBH-20.csv', dtype=dtypes, low_memory=False)
  df2 = pd.read_csv('srbh_training/SRBH_2020.csv', dtype=dtypes, low_memory=False)
  
  print(f"SRBH-20.csv: {len(df1)} rows")
  print(f"SRBH_2020.csv: {len(df2)} rows")
  
  # Combine
  df_combined = pd.concat([df1, df2], ignore_index=True)
  df_combined = df_combined.drop_duplicates()
  
  print(f"Combined dataset: {len(df_combined)} rows")
  print(f"Memory usage: {df_combined.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
  
  # Save optimized version
  output_path = 'srbh_training/srbh_processed_pi.csv'
  df_combined.to_csv(output_path, index=False, compression='gzip')
  
  file_size = os.path.getsize(output_path) / 1024**2
  print(f"Saved to {output_path}")
  print(f"Compressed size: {file_size:.2f} MB")
  print("Done!")

Save: Ctrl+X, Y, Enter

Run preprocessing:
  $ python3 preprocess_for_pi.py

This creates compressed dataset optimized for Pi's memory.

================================================================================
PART 6: CONFIGURE FOR RASPBERRY PI DEPLOYMENT
================================================================================

6.1 OPTIMIZE ANALYZER SCRIPTS
-----------------------------

Edit improved_results_analyzer.py for Pi:
  $ nano improved_results_analyzer.py

Find the section with:
  n_bootstraps = 1000

Change to:
  n_bootstraps = 500  # Reduced for Pi performance

This reduces bootstrap iterations from 1000 to 500, cutting analysis time in half
while maintaining accuracy.

6.2 CONFIGURE FLASK APP FOR PI
------------------------------

Edit app.py:
  $ nano app.py

Find:
  if __name__ == '__main__':
      app.run(debug=True, host='0.0.0.0', port=5000)

Change to:
  if __name__ == '__main__':
      app.run(debug=False, host='0.0.0.0', port=5000, threaded=True)

This optimizes Flask for Raspberry Pi:
  ‚Ä¢ debug=False: Reduces memory usage
  ‚Ä¢ threaded=True: Better multi-request handling

6.3 CREATE PI-SPECIFIC CONFIG
-----------------------------

Create config file:
  $ nano ~/ids-system/pi_config.json

Add:
  {
    "hardware": {
      "device": "raspberry_pi_5",
      "ram_gb": 8,
      "cores": 4,
      "storage": "microsd"
    },
    "optimization": {
      "batch_size": 1000,
      "bootstrap_samples": 500,
      "parallel_jobs": 2
    },
    "monitoring": {
      "log_directory": "/home/pi/ids-system/logs",
      "retention_days": 30,
      "alert_threshold": 0.45
    },
    "network": {
      "bind_address": "0.0.0.0",
      "port": 5000,
      "ssl_enabled": false
    }
  }

================================================================================
PART 7: RUN ANALYSIS ON RASPBERRY PI
================================================================================

7.1 FIRST RUN: BASELINE GENERATION
----------------------------------

Connect via SSH and run:
  $ cd ~/ids-system
  $ source ids_env/bin/activate
  
  $ python3 generate_baseline.py

Expected output:
  Loading SRBH data...
  Loaded 24,990 records
  Calculating Gaussian baseline...
  Baseline statistics:
    Mean drift score: 1.1487
    Std dev: 0.3088
  Saved to: baseline.csv
  Done! [XX seconds]

On Pi 5: ~10-20 seconds

7.2 GENERATE DRIFT LOG
---------------------

Run:
  $ python3 -c "
  import pandas as pd
  import numpy as np
  
  # Create realistic drift log from SRBH
  df = pd.read_csv('srbh_training/SRBH-20.csv')
  df['timestamp'] = pd.date_range('2026-01-30', periods=len(df), freq='2S')
  df['drift_score'] = np.random.normal(1.5, 0.4, len(df))
  df['alert_level'] = df['drift_score'].apply(
      lambda x: 'ALERT' if x > 1.5 else ('WARNING' if x > 0.5 else 'NORMAL')
  )
  df.to_csv('drift_log.csv', index=False)
  print(f'Created drift_log.csv with {len(df)} records')
  "

Or simpler: Copy from main computer:
  From main computer PowerShell:
    $ scp C:\Users\kaush\Downloads\ids-system\drift_log.csv pi@ids-pi.local:~/ids-system/

7.3 RUN ANALYZER 1: IMPROVED RESULTS
------------------------------------

Run:
  $ python3 improved_results_analyzer.py

Expected output (on Pi 5):
  Loading drift log data...
  Preparing data for analysis...
  Calculating 15 metrics...
  Computing 95% bootstrap confidence intervals (500 samples)...
  Analyzing by phase...
  Generating visualizations...
  
  ‚úì Accuracy: 85.03% [84.59%-85.47%]
  ‚úì Recall: 100.00%
  ‚úì ROC-AUC: 100.00%
  ‚úì F1-Score: 57.33%
  
  Output files:
    ‚Ä¢ improved_metrics_summary.csv
    ‚Ä¢ improved_phase_analysis.csv
    ‚Ä¢ improved_results_detailed.csv
    ‚Ä¢ improved_roc_curve.png
    ‚Ä¢ improved_confusion_matrix.png
    ‚Ä¢ improved_drift_distribution.png
    ‚Ä¢ improved_metrics_comparison.png
    ‚Ä¢ improved_precision_recall_curve.png
  
  Done! [XX seconds]

On Pi 5: ~30-45 seconds (slower than desktop, but acceptable)

7.4 RUN ANALYZER 2: THRESHOLD OPTIMIZER
---------------------------------------

Run:
  $ python3 advanced_threshold_optimizer.py

Expected output:
  Optimizing thresholds with 5 strategies...
  
  F1-Score Optimization: 0.4500
  Youden Index: 0.4501
  Balanced Accuracy: 0.4500
  Cost-Sensitive: 0.4500
  ROC Optimal: 0.4501
  
  ENSEMBLE RECOMMENDATION: 0.4500
  
  Output files:
    ‚Ä¢ optimized_thresholds.json
    ‚Ä¢ threshold_optimization_curves.png
  
  Done! [XX seconds]

On Pi 5: ~15-20 seconds

7.5 RUN ANALYZER 3: COMPREHENSIVE REPORT
----------------------------------------

Run:
  $ python3 comprehensive_accuracy_report.py

Expected output:
  Running statistical analysis...
  t-test: t=-196.58, p<0.001
  KS-test: KS=1.00, p<0.001
  Cohen's d: 3.4446
  
  Attack Pattern Detection:
    High drift samples: 108
    Alert bursts: 2,286
    /login attack rate: 99.70%
    /api/data attack rate: 9.94%
  
  Detection Efficiency:
    Time to Detection: 0 samples
    Alert Purity: 54.01%
    Attack Coverage: 100.00%
  
  Output files:
    ‚Ä¢ comprehensive_accuracy_report.json
    ‚Ä¢ comprehensive_accuracy_analysis.png
  
  Done! [XX seconds]

On Pi 5: ~20-30 seconds

7.6 TOTAL ANALYSIS TIME ON PI
----------------------------

Summary:
  ‚îú‚îÄ Baseline generation: 10-20 seconds
  ‚îú‚îÄ Analyzer 1: 30-45 seconds
  ‚îú‚îÄ Analyzer 2: 15-20 seconds
  ‚îú‚îÄ Analyzer 3: 20-30 seconds
  ‚îî‚îÄ TOTAL: ~75-115 seconds (~1.5-2 minutes)

Acceptable for edge deployment on Raspberry Pi 5!

For comparison:
  ‚Ä¢ Desktop (Windows 10): 10-30 seconds total
  ‚Ä¢ Pi 5: 75-115 seconds total
  ‚Ä¢ Ratio: ~5-8x slower (expected for ARM processor)

================================================================================
PART 8: REAL-TIME MONITORING ON PI
================================================================================

8.1 RUN FLASK APP ON PI
----------------------

Start Flask app:
  $ python3 app.py

Expected output:
   * Running on http://0.0.0.0:5000
   * Debug mode: off

Access from another computer on same network:
  ‚Ä¢ Browser: http://ids-pi.local:5000
  ‚Ä¢ Or: http://192.168.x.x:5000 (replace with Pi's IP)

Find Pi's IP address (on Pi terminal):
  $ hostname -I

Example: 192.168.1.100

8.2 CONFIGURE SYSTEMD SERVICE (RUN ON BOOT)
--------------------------------------------

Create service file:
  $ sudo nano /etc/systemd/system/ids-app.service

Paste:
  [Unit]
  Description=IDS Flask Application
  After=network.target
  
  [Service]
  Type=simple
  User=pi
  WorkingDirectory=/home/pi/ids-system
  Environment="PATH=/home/pi/ids-system/ids_env/bin"
  ExecStart=/home/pi/ids-system/ids_env/bin/python3 app.py
  Restart=always
  RestartSec=5
  
  [Install]
  WantedBy=multi-user.target

Save: Ctrl+X, Y, Enter

Enable service:
  $ sudo systemctl enable ids-app.service

Start service:
  $ sudo systemctl start ids-app.service

Check status:
  $ sudo systemctl status ids-app.service

View logs:
  $ sudo journalctl -u ids-app.service -f

Now Flask app runs automatically on Pi boot!

8.3 CONFIGURE DRIFT DETECTOR AS SERVICE
---------------------------------------

Create service for continuous monitoring:
  $ sudo nano /etc/systemd/system/ids-monitor.service

Paste:
  [Unit]
  Description=IDS Drift Detector Monitor
  After=network.target ids-app.service
  
  [Service]
  Type=simple
  User=pi
  WorkingDirectory=/home/pi/ids-system
  Environment="PATH=/home/pi/ids-system/ids_env/bin"
  ExecStart=/home/pi/ids-system/ids_env/bin/python3 online_monitor.py
  Restart=always
  RestartSec=5
  
  [Install]
  WantedBy=multi-user.target

Enable and start:
  $ sudo systemctl enable ids-monitor.service
  $ sudo systemctl start ids-monitor.service

Check status:
  $ sudo systemctl status ids-monitor.service

8.4 MONITORING DASHBOARD
------------------------

Create monitoring script:
  $ nano ~/ids-system/monitor_pi.py

Paste:
  #!/usr/bin/env python3
  """
  Monitor IDS system on Raspberry Pi
  Shows CPU, memory, disk usage, and IDS status
  """
  
  import os
  import psutil
  import json
  import subprocess
  from datetime import datetime
  
  def get_system_stats():
      """Get Pi system statistics"""
      return {
          'timestamp': datetime.now().isoformat(),
          'cpu_percent': psutil.cpu_percent(interval=1),
          'memory_percent': psutil.virtual_memory().percent,
          'disk_percent': psutil.disk_usage('/').percent,
          'temperature': get_cpu_temp(),
          'uptime_seconds': int(os.popen('uptime -s').read().strip()),
      }
  
  def get_cpu_temp():
      """Get Pi CPU temperature"""
      try:
          with open('/sys/class/thermal/thermal_zone0/temp', 'r') as f:
              return int(f.read()) / 1000.0
      except:
          return None
  
  def get_ids_status():
      """Check IDS services status"""
      services = ['ids-app.service', 'ids-monitor.service']
      status = {}
      for service in services:
          result = subprocess.run(
              ['systemctl', 'is-active', service],
              capture_output=True, text=True
          )
          status[service] = result.stdout.strip()
      return status
  
  def main():
      print("=" * 60)
      print("RASPBERRY PI 5 - IDS SYSTEM MONITORING")
      print("=" * 60)
      
      # System stats
      stats = get_system_stats()
      print("\nSYSTEM STATS:")
      print(f"  CPU Usage: {stats['cpu_percent']}%")
      print(f"  Memory Usage: {stats['memory_percent']}%")
      print(f"  Disk Usage: {stats['disk_percent']}%")
      print(f"  Temperature: {stats['temperature']}¬∞C")
      
      # IDS status
      ids_status = get_ids_status()
      print("\nIDS SERVICES:")
      for service, status in ids_status.items():
          print(f"  {service}: {status}")
      
      # Recommendations
      print("\nRECOMMENDATIONS:")
      if stats['cpu_percent'] > 80:
          print("  ‚ö† CPU usage high - consider reducing analysis frequency")
      if stats['temperature'] > 60:
          print("  ‚ö† Temperature high - ensure proper ventilation")
      if stats['disk_percent'] > 90:
          print("  ‚ö† Disk space low - clean old logs")
      
      print("\n" + "=" * 60)
  
  if __name__ == '__main__':
      main()

Make executable:
  $ chmod +x ~/ids-system/monitor_pi.py

Run:
  $ python3 monitor_pi.py

This shows system health!

================================================================================
PART 9: DATA LOGGING & PERSISTENCE
================================================================================

9.1 SET UP LOG DIRECTORY
------------------------

Create logs directory:
  $ mkdir -p ~/ids-system/logs

Configure log rotation:
  $ sudo nano /etc/logrotate.d/ids-system

Paste:
  /home/pi/ids-system/logs/*.log {
      daily
      rotate 30
      compress
      delaycompress
      notifempty
      missingok
      create 0640 pi pi
  }

This keeps logs for 30 days, compresses old ones.

9.2 STORE ANALYSIS RESULTS
--------------------------

Create results archival script:
  $ nano ~/ids-system/archive_results.sh

Paste:
  #!/bin/bash
  
  # Archive analysis results
  DATE=$(date +%Y%m%d_%H%M%S)
  ARCHIVE_DIR="/home/pi/ids-system/results/archive"
  
  mkdir -p $ARCHIVE_DIR
  
  # Tar and compress results
  tar -czf $ARCHIVE_DIR/results_$DATE.tar.gz \
    improved_*.csv \
    optimized_thresholds.json \
    comprehensive_*.json \
    *.png
  
  echo "Results archived to: $ARCHIVE_DIR/results_$DATE.tar.gz"
  
  # Clean up originals (keep last 7 days)
  find $ARCHIVE_DIR -name "results_*.tar.gz" -mtime +7 -delete

Make executable:
  $ chmod +x ~/ids-system/archive_results.sh

Schedule with cron (daily archival):
  $ crontab -e

Add line:
  0 2 * * * ~/ids-system/archive_results.sh

This runs archive script daily at 2 AM.

================================================================================
PART 10: PRODUCTION DEPLOYMENT CHECKLIST
================================================================================

10.1 PRE-DEPLOYMENT VERIFICATION
-------------------------------

Before deploying to production, verify:

‚òê System Setup
  ‚òê Raspberry Pi 5 boots successfully
  ‚òê Disk space adequate (>10 GB free)
  ‚òê RAM available (>2 GB free)
  ‚òê Temperature < 70¬∞C
  ‚òê Network connectivity working (Ethernet or WiFi)

‚òê Software Installation
  ‚òê Python 3.11+ installed
  ‚òê Virtual environment created
  ‚òê All dependencies installed (pandas, numpy, etc.)
  ‚òê Project files copied successfully
  ‚òê SRBH dataset present

‚òê Analysis Verification
  ‚òê Baseline generation completes (~20 sec)
  ‚òê Analyzer 1 completes (~45 sec)
  ‚òê Analyzer 2 completes (~20 sec)
  ‚òê Analyzer 3 completes (~30 sec)
  ‚òê All output files created (15+ files)
  ‚òê Results show expected metrics:
    ‚îú‚îÄ Accuracy ~85%
    ‚îú‚îÄ Recall 100%
    ‚îú‚îÄ ROC-AUC 100%
    ‚îî‚îÄ Optimal threshold 0.45

‚òê Flask App Verification
  ‚òê App starts without errors
  ‚òê Accessible from browser (http://ids-pi.local:5000)
  ‚òê Endpoints responding

‚òê Service Configuration
  ‚òê ids-app.service created and enabled
  ‚òê ids-monitor.service created and enabled
  ‚òê Services start on boot
  ‚òê Logs are rotating properly

‚òê Monitoring Setup
  ‚òê Monitor script runs successfully
  ‚òê Shows system stats and service status
  ‚òê Temperature within normal range
  ‚òê CPU usage reasonable

10.2 DEPLOYMENT STEPS
--------------------

1. Final System Check
   $ python3 monitor_pi.py
   
2. Start Services
   $ sudo systemctl restart ids-app.service
   $ sudo systemctl restart ids-monitor.service
   
3. Verify Services Running
   $ sudo systemctl status ids-app.service
   $ sudo systemctl status ids-monitor.service
   
4. Test Web Interface
   ‚Ä¢ Open browser: http://ids-pi.local:5000
   ‚Ä¢ Should load without errors
   
5. Monitor First 24 Hours
   ‚Ä¢ Check logs for errors
   ‚Ä¢ Verify alert generation
   ‚Ä¢ Confirm threshold (0.45) being used
   
6. Enable Auto-start (if not done)
   $ sudo systemctl enable ids-app.service
   $ sudo systemctl enable ids-monitor.service

10.3 HEALTH CHECKS (ONGOING)
---------------------------

Daily:
  $ python3 monitor_pi.py
  (Check for warnings)

Weekly:
  $ df -h
  (Check disk space)
  
  $ sudo systemctl status ids-*.service
  (Check service status)

Monthly:
  $ tar -czf monthly_backup_$(date +%Y%m).tar.gz results/ logs/
  (Backup results and logs)

================================================================================
PART 11: TROUBLESHOOTING ON RASPBERRY PI
================================================================================

11.1 COMMON ISSUES
-----------------

Issue 1: "ModuleNotFoundError: No module named 'pandas'"
  Solution:
    $ source ids_env/bin/activate
    $ pip install pandas
  
  Cause: Virtual environment not activated

Issue 2: "PermissionError: Permission denied"
  Solution:
    $ chmod +x script_name.sh
    $ chmod 755 ~/ids-system
  
  Cause: Incorrect file permissions

Issue 3: "Memory error" or "Killed" (SIGKILL)
  Solution 1:
    ‚Ä¢ Reduce bootstrap_samples in analyzer (500 instead of 1000)
    ‚Ä¢ Reduce batch size
    
  Solution 2:
    ‚Ä¢ Add swap space:
      $ sudo fallocate -l 2G /swapfile
      $ sudo chmod 600 /swapfile
      $ sudo mkswap /swapfile
      $ sudo swapon /swapfile
  
  Cause: Insufficient RAM (unlikely with 8GB)

Issue 4: Slow Analysis (>2 minutes)
  Solution:
    ‚Ä¢ Use USB SSD instead of microSD
    ‚Ä¢ Disable visualization generation (PNG files)
    ‚Ä¢ Reduce dataset size
  
  Cause: microSD card I/O bottleneck

Issue 5: "Connection refused" accessing Flask app
  Solution:
    ‚Ä¢ Check app is running: $ systemctl status ids-app.service
    ‚Ä¢ Check port 5000: $ sudo netstat -tulpn | grep 5000
    ‚Ä¢ Try localhost: http://localhost:5000 (on Pi terminal)
  
  Cause: App not started or port blocked

11.2 PERFORMANCE OPTIMIZATION
----------------------------

If analysis is slow:

1. Enable GPU acceleration (if available):
   $ pip install tensorflow-lite
   (Raspberry Pi 5 has optional GPU support)

2. Reduce precision for faster computation:
   ‚Ä¢ Use float32 instead of float64
   ‚Ä¢ Reduce sample count in optimization

3. Increase swap (if needed):
   $ sudo dphys-swapfile swapoff
   $ sudo nano /etc/dphys-swapfile
   (Change CONF_SWAPSIZE=100 to CONF_SWAPSIZE=2048)
   $ sudo dphys-swapfile swapon

4. Enable CPU frequency scaling:
   $ echo "powersave" | sudo tee /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor
   (For thermal management)

11.3 MONITORING RESOURCES
------------------------

Real-time monitoring:
  $ watch -n 1 'free -h && echo "---" && df -h'

Detailed process info:
  $ top
  (Press 'q' to exit)

CPU temperature (continuous):
  $ watch -n 1 'echo "CPU Temp: $(cat /sys/class/thermal/thermal_zone0/temp | awk '"'"'{print $1/1000}'"'"')¬∞C"'

================================================================================
PART 12: ADVANCED CONFIGURATION
================================================================================

12.1 ENABLE SSH KEY-BASED AUTHENTICATION
---------------------------------------

On your main computer, generate key pair:
  Windows (PowerShell):
    $ ssh-keygen -t rsa -b 4096

  Mac/Linux:
    $ ssh-keygen -t rsa -b 4096

Copy public key to Pi:
  $ ssh-copy-id -i ~/.ssh/id_rsa.pub pi@ids-pi.local

Now login without password:
  $ ssh pi@ids-pi.local

12.2 SETUP REMOTE BACKUP
------------------------

Backup results to main computer automatically:

On Pi, create backup script:
  $ nano ~/ids-system/backup_to_main.sh

Paste:
  #!/bin/bash
  
  # Backup results to main computer
  BACKUP_USER="kaush"
  BACKUP_HOST="192.168.1.100"  # Main computer IP
  BACKUP_PATH="/backup/ids-results"
  
  rsync -avz ~/ids-system/results/ \
    $BACKUP_USER@$BACKUP_HOST:$BACKUP_PATH/
  
  echo "Backup completed to $BACKUP_HOST:$BACKUP_PATH"

Schedule (daily at 3 AM):
  $ crontab -e
  
  Add:
    0 3 * * * ~/ids-system/backup_to_main.sh

12.3 EMAIL ALERTS FOR CRITICAL EVENTS
-------------------------------------

Install mail utility:
  $ sudo apt install -y mailutils

Create alert script:
  $ nano ~/ids-system/alert_on_critical.py

Paste:
  #!/usr/bin/env python3
  
  import os
  import smtplib
  from email.mime.text import MIMEText
  
  def send_alert(subject, message):
      sender = "pi@ids-pi.local"
      recipient = "your.email@gmail.com"
      
      msg = MIMEText(message)
      msg['Subject'] = subject
      msg['From'] = sender
      msg['To'] = recipient
      
      with smtplib.SMTP('localhost') as server:
          server.send_message(msg)
  
  # Example: Alert if attacks detected
  import json
  try:
      with open('optimized_thresholds.json') as f:
          results = json.load(f)
          if results['attacks_detected'] > 100:
              send_alert(
                  "IDS Alert: High Attack Rate",
                  f"Attacks detected: {results['attacks_detected']}"
              )
  except Exception as e:
      print(f"Alert failed: {e}")

Make executable:
  $ chmod +x ~/ids-system/alert_on_critical.py

================================================================================
PART 13: PERFORMANCE BENCHMARKS ON RASPBERRY PI 5
================================================================================

13.1 EXPECTED PERFORMANCE
------------------------

Analysis Times on Pi 5 (8GB, Debian):

  Operation                Time        Notes
  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Baseline generation      10-20 sec   Depends on disk speed
  Analyzer 1 (15 metrics)  30-45 sec   Bootstrap CI takes time
  Analyzer 2 (5 strategies) 15-20 sec   Threshold search
  Analyzer 3 (statistics)  20-30 sec   Statistical tests
  Full analysis            75-115 sec  ~1.5-2 minutes
  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Real-time Monitoring:
  ‚Ä¢ Single record processing: < 1 ms
  ‚Ä¢ Batch (1000 records): ~500-1000 ms
  ‚Ä¢ Requests/sec capacity: 10-20 req/sec

Comparison to Desktop:
  ‚Ä¢ Desktop: ~15 seconds total analysis
  ‚Ä¢ Raspberry Pi 5: ~100 seconds
  ‚Ä¢ Slowdown factor: ~6-7x (acceptable for edge)

13.2 MEMORY USAGE
-----------------

Typical memory allocation:
  ‚Ä¢ OS + system: 400-600 MB
  ‚Ä¢ Python runtime: 150-200 MB
  ‚Ä¢ pandas DataFrame (24k rows): 100-150 MB
  ‚Ä¢ Analysis runtime: 50-100 MB
  ‚Ä¢ Total: ~700-1000 MB (~10% of 8GB)

Very safe with 8GB RAM!

13.3 DISK USAGE
---------------

Disk space required:
  ‚Ä¢ OS + system: 3-4 GB
  ‚Ä¢ Python environment: 300-500 MB
  ‚Ä¢ Project code: 50 MB
  ‚Ä¢ SRBH dataset: 200 MB (compressed: 80 MB)
  ‚Ä¢ Analysis results (per run): 5-10 MB
  ‚Ä¢ Monthly logs: 100-200 MB
  
  Total: ~4.5-5 GB for setup + 6+ months of data

64GB microSD card is plenty!

================================================================================
PART 14: FINAL DEPLOYMENT CHECKLIST
================================================================================

14.1 COMPLETE DEPLOYMENT WORKFLOW
--------------------------------

Step 1: Hardware Setup (5 minutes)
  ‚òê Insert microSD card into Pi
  ‚òê Connect Ethernet cable
  ‚òê Connect USB-C power
  ‚òê Wait for boot

Step 2: OS Installation (20 minutes)
  ‚òê SSH into Pi
  ‚òê Run system updates
  ‚òê Install essential tools

Step 3: Python Environment (15 minutes)
  ‚òê Create virtual environment
  ‚òê Install dependencies (pandas, numpy, etc.)
  ‚òê Verify installations

Step 4: Project Deployment (10 minutes)
  ‚òê Transfer project files (SCP or USB)
  ‚òê Transfer SRBH dataset
  ‚òê Verify all files present

Step 5: Initial Testing (10 minutes)
  ‚òê Run baseline generation
  ‚òê Run all 3 analyzers
  ‚òê Verify output files

Step 6: Service Configuration (5 minutes)
  ‚òê Create systemd services
  ‚òê Enable auto-start
  ‚òê Start services

Step 7: Verification & Optimization (10 minutes)
  ‚òê Run monitor script
  ‚òê Check system stats
  ‚òê Verify Flask app accessible
  ‚òê Test alert generation

TOTAL TIME: ~75 minutes (~1.25 hours)

14.2 POST-DEPLOYMENT TASKS
-------------------------

First Week:
  ‚Ä¢ Monitor system 24/7 for any issues
  ‚Ä¢ Check logs daily for errors
  ‚Ä¢ Verify alerts are being generated
  ‚Ä¢ Confirm threshold (0.45) is active

First Month:
  ‚Ä¢ Collect baseline performance metrics
  ‚Ä¢ Identify any edge cases
  ‚Ä¢ Optimize parameters if needed
  ‚Ä¢ Plan long-term monitoring strategy

Ongoing:
  ‚Ä¢ Weekly health checks
  ‚Ä¢ Monthly data backups
  ‚Ä¢ Quarterly analysis re-runs
  ‚Ä¢ Annual security audits

================================================================================
PART 15: SUMMARY & QUICK REFERENCE
================================================================================

15.1 KEY COMMANDS REFERENCE
---------------------------

General Commands:
  $ ssh pi@ids-pi.local              # Connect to Pi
  $ exit                              # Disconnect
  $ free -h                           # Check memory
  $ df -h                             # Check disk
  $ ps aux | grep python             # Check processes

Virtual Environment:
  $ source ids_env/bin/activate      # Activate venv
  $ deactivate                        # Deactivate venv
  $ pip list                          # List packages

Analysis:
  $ python3 improved_results_analyzer.py
  $ python3 advanced_threshold_optimizer.py
  $ python3 comprehensive_accuracy_report.py

Services:
  $ sudo systemctl status ids-app.service
  $ sudo systemctl start ids-app.service
  $ sudo systemctl stop ids-app.service
  $ sudo systemctl restart ids-app.service
  $ sudo systemctl enable ids-app.service    # Auto-start
  $ sudo journalctl -u ids-app.service -f    # View logs

Monitoring:
  $ python3 monitor_pi.py
  $ top
  $ df -h

15.2 PROJECT STRUCTURE ON PI
----------------------------

~/ids-system/
‚îú‚îÄ‚îÄ ids_env/                          # Virtual environment
‚îú‚îÄ‚îÄ app.py                            # Flask app
‚îú‚îÄ‚îÄ drift_detector.py                 # Drift detection
‚îú‚îÄ‚îÄ generate_baseline.py              # Baseline generator
‚îú‚îÄ‚îÄ online_monitor.py                 # Real-time monitor
‚îú‚îÄ‚îÄ improved_results_analyzer.py      # Analyzer 1
‚îú‚îÄ‚îÄ advanced_threshold_optimizer.py   # Analyzer 2
‚îú‚îÄ‚îÄ comprehensive_accuracy_report.py  # Analyzer 3
‚îú‚îÄ‚îÄ srbh_training/
‚îÇ  ‚îú‚îÄ‚îÄ SRBH-20.csv                   # Dataset
‚îÇ  ‚îú‚îÄ‚îÄ SRBH_2020.csv                 # Dataset
‚îÇ  ‚îî‚îÄ‚îÄ srbh_processed.csv            # Preprocessed
‚îú‚îÄ‚îÄ results/                          # Analysis outputs
‚îÇ  ‚îú‚îÄ‚îÄ *.csv                         # Data files
‚îÇ  ‚îú‚îÄ‚îÄ *.json                        # Results
‚îÇ  ‚îî‚îÄ‚îÄ *.png                         # Charts
‚îú‚îÄ‚îÄ logs/                             # System logs
‚îÇ  ‚îî‚îÄ‚îÄ *.log                         # Log files
‚îî‚îÄ‚îÄ fullproject.txt                   # This documentation

15.3 SUCCESS INDICATORS
----------------------

After deployment, verify:

‚úì Flask app running: Visit http://ids-pi.local:5000
‚úì Analysis completes: Check results/ folder has files
‚úì Services active: $ sudo systemctl status ids-app.service
‚úì Monitoring works: $ python3 monitor_pi.py
‚úì CPU temp normal: < 70¬∞C
‚úì Disk usage acceptable: > 10 GB free
‚úì Memory usage low: < 50% of 8GB

All green? You're ready for production!

================================================================================
CONCLUSION
================================================================================

You now have a complete guide to deploy the IDS system on Raspberry Pi 5 with
8GB RAM running Debian.

Key takeaways:
  ‚úì Pi 5 has sufficient hardware for this workload
  ‚úì Total deployment time: ~75 minutes
  ‚úì Analysis time: ~100 seconds (acceptable for edge)
  ‚úì Memory usage: ~1 GB (10% of available)
  ‚úì Disk usage: ~5-6 GB for full setup + 6 months data
  ‚úì Production-ready after following this guide

Next Steps:
  1. Prepare Raspberry Pi 5 and microSD card
  2. Follow Part 2-4 for initial setup
  3. Run Part 7 for analysis testing
  4. Complete Part 10 deployment checklist
  5. Start Part 8 for ongoing monitoring

Questions? Refer back to specific sections or troubleshooting guide (Part 11).

Good luck with your edge IDS deployment! üöÄ

================================================================================
END OF RASPBERRY PI 5 DEPLOYMENT GUIDE
================================================================================
